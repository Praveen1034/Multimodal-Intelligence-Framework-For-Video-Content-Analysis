{
  "pip_location": [],
  "sequence": [
    {
      "type": "slide",
      "start_time": "0.0:0.0:00",
      "start_index": 0,
      "offset": 0.0,
      "source": "Output\\Distillation_Robustifies_Unlearning\\thumbnails\\hi\\thumb-0001.png",
      "text_ocr": "What specific problem does the pro- posed UNDO method aim to address in the context of large language mod- els, and why is this problem significant?",
      "selected_image": "Output\\Distillation_Robustifies_Unlearning\\Selected\\thumb-0001.png"
    },
    {
      "type": "slide",
      "start_time": "0.0:0.0:13",
      "start_index": 7,
      "offset": 13.913454545454544,
      "source": "Output\\Distillation_Robustifies_Unlearning\\thumbnails\\hi\\thumb-0008.png",
      "text_ocr": "Model = i Modals remain, Figure |: Existing LLM methods suppress undesired but are reversible using a small amount of finetuning. We show that distilling the suppressed into a randomly initialized network significantly increases resilience against reacquiring the undesired behavior. Our method substantially outperforms other robust unlearning baselines, including RepNoise [59] and SAM [22]. capabilities are left behind (Figure 1). Across diverse domains, including language, arithmetic, and of mass destruction, we find consistently improves resistance to adversarial relearning attacks. Remarkably, distilled models relearn harmful capabilities at rates comparable to oracle model trained from scratch with data This finding has immediate practical implications. Distillation is already standard practice in many contexts [29, 82]. For example, LLM developers distill models to reduce inference costs 72] By applying methods before these developers can produce models that are strongly resistant to relearning attacks. Building on this insight, we introduce UNDO (Unlearn-Noise-Distill-on-Outputs), which distills an model into a partially noised copy of itself, enabling a tradeoff between compute cost and robustness. With this framework, we approximate the robustness of gold-standard data filtering at a fraction of the cost, offering frontier in safe and scalable model We summarize our contributions as follows: 3, we provide evidence of a limitation in behavioral training for robust unlearning in LLMs. Even when achieving perfect behavioral agreement with an unlearning oracle models retain latent that can be easily restored through finetuning. In Section 4, we show that distillation Training a new, randomly initialized \u2018model on the outputs of an unlearned will transfer desired behavior while leaving latent capabilities behind, In Section 5, we propose UNDO, which an outputs into a noised copy of itself. This method enables a novel compute-robustness tradcoff that interpolates between mere suppression and fully robust We benchmark UNDO against a variety of methods on language and arithmetic tasks, extending the Pareto frontier of retain performance versus forget unlearning robustness. We also show competitive performance and robustification of unlearning on the WMDP benchmark [40]. Robust Unlearning Machine to remove specified knowledge from a trained model while preserving the overall functionality. This goal has been operationalized in different ways (39]. We follow recent work with removing undesired capabilities from LLMs in a way that is robust to adversarial 68]. We consider the following problem: Robust A robust unlearning method accepts the following inputs: a Reference neural network with be removed; Ser: a collection of examples desired capabilities; (ii) a Forget Set: of examples demonstrating undesired capabilities; (iv) Training Data: a collection of unlabeled examples. The method produces a new that is evaluated according to (a) a Retain Evaluation: a measure of the desired",
      "selected_image": "Output\\Distillation_Robustifies_Unlearning\\Selected\\thumb-0008.png"
    },
    {
      "type": "slide",
      "start_time": "0.0:1.0:01",
      "start_index": 31,
      "offset": 61.61672727272727,
      "source": "Output\\Distillation_Robustifies_Unlearning\\thumbnails\\hi\\thumb-0032.png",
      "text_ocr": "Distillation Robustifies Unlearning Bruce W. Leni Bryce Matt Turner Alignment & Theory of Pennsylvania of Technology University Abstract LLM unlearning methods are not robust: they can be reverted easily with a few steps of finetuning. This is true even for the idealized method of training to imitate an oracle model that was never exposed to unwanted information, suggesting that output-based is insufficient to achieve robust In a similar vein, we find that training a randomly initialized student to imitate an model transfers desired behaviors while leaving undesired capabilities behind. In other words, distillation robustifies Building on this insight, we propose (UNDO), method that distills an unlearned mode! into a partially noised copy of itself. UNDO introduces a tunable tradeoff between compute cost and robustness, establishing a new Pareto frontier on synthetic and arithmetic tasks. its strongest setting, UNDO the robustness of a model retrained from with perfect data while using only 60-80% of the compute and requiring only 0.01% of the pretraining data to be labeled. We also show that UNDO robustifies unlearning on the more realistic Weapons of Mass Destruction Proxy benchmark. Since distillation is widely used in practice, an unlearning step beforehand offers a convenient path to robust capability removal. Large language models can acquire harmful capabilities during pretraining on massive, datasets 62]. This complicates model deployment. For example, a mode! with knowledge relevant to developing novel cyberweapons could enable global-scale harm if accessed by bad actors [52, 61]. While data filtering before pretraining could mitigate such risks, precisely auditing and filtering data at the required scale remains impractical 81, 1] Post-training methods like Reinforcement learning from Human Feedback discourage models from using undesired capabilities on common inputs 53], but the underlying capabilities intact. a result, the model remains vulnerable to attacks that can elicit these capabilities, including adversarial or direct finetuning 36, 80]. To address this vulnerability, recent has tried using machine unlearning to remove undesired capabilities from LLMs 66, 6]. However, existing unlearning methods also suppress capabilities rather than remove them [30, \u2018The supposedly unlearned capabilities can be restored through a few steps of finetuning, leaving the fundamental challenge of achieving true capability removal unsolved 47, 22, 31]. We present a simple but powerful observation: When an unlearned model is into a randomly initialized student, desired behavior while undesired to edu. Contributors. during the MATS program. Author contributions in Appendix F. share our code implementation publicly through Preprint. Under review:",
      "selected_image": "Output\\Distillation_Robustifies_Unlearning\\Selected\\thumb-0032.png"
    },
    {
      "type": "slide",
      "start_time": "0.0:1.0:39",
      "start_index": 50,
      "offset": 99.38181818181818,
      "source": "Output\\Distillation_Robustifies_Unlearning\\thumbnails\\hi\\thumb-0051.png",
      "text_ocr": "Figure 4: Unlearn-and-Distill boosts robustness to (a-c) trends for the forget domain (Korean), comparing unlearning-only methods MaxEnt, RMU) against models with an additional distillation step, measured gold standard of We highlight the least favorable learning curve for each method. (d-f) Relearning trends for the arithmetic forget domain (Multiplication & plasticity [45] and representational entanglement 48]. Our experiments confirm that when we standard unleaming methods to a model, the resulting model quickly under However, when we distill this model into a randomly initialized student, the student substantially slower while maintaining the same unlearning behavior. Trading Compute for Unlearning Robustness So far, we have demonstrated that an unlearned model into a randomly initialized model significantly enhances robustness against relearning attacks. However, this approach substantial computational costs, as We are training a student model from scratch. Moving beyond the basic method established in Section 4, we now investigate whether the method be adapted to achieve different levels of robustness with different amounts of compute. That is, we trade off a bit of robustness to reduce compute costs, or vice versa? In this section, we conduct experiments that generalize the approach in Section 4 a three-step process. create a behaviorally suppressed model (teacher) by applying standard methods. (ii) Noise: create a student model by perturbing the weights of the suppressed model, damaging it. repair this damaged student by distilling to recover the original behavior. The controlled perturbation enables us to interpolate between the original and full We refer to this method as Unlearn-Noise-Distill-on-Outputs, \u2018or simply, UNDO. We define the perturbed model as: = where mixing coefficient a noise scale R*, and N represents sampled noise. We sample using Xavier though other random noise sampling methods could be Intuitively, the term Shrinks the suppressed parameters, and the term injects noise. As we from 1, we can View interpolating between the paramterization and fuil We observe later in Section 6 that simply shrinking the parameters (setting = 0) can also be effective, as the key idea globally damage the network to varying degrees, though including adds expressivity to the formulation. Note that when 1 and the perturbation reduces to random initialization. Experimental Setup. We start by perturbing the models that are suppressed with MaxEnt from Section 4 and follow the same protocols for using forward KL divergence on the outputs. For both language and arithmetic settings, we experiment with various values! of while fixing We test 0.55, 0.6,0.65, for language and",
      "selected_image": "Output\\Distillation_Robustifies_Unlearning\\Selected\\thumb-0051.png"
    },
    {
      "type": "slide",
      "start_time": "0.0:5.0:06",
      "start_index": 154,
      "offset": 306.09599999999995,
      "source": "Output\\Distillation_Robustifies_Unlearning\\thumbnails\\hi\\thumb-0155.png",
      "text_ocr": "(b) an Elicited Forget Evaluation: a measure of the latent undesired capabilities. \u2018The goal is to produce a with high retain performance and low elicited forget performance. Elicitation We use finetuning on the forget set, commonly referred to as attack, as our elicitation method. attacks have repeatedly proven to be among the most effective ways to elicit unlearned knowledge or capabilities in supposedly unlearned models recent studies treat resistance to such finetuning-based probes the primary criterion for robust 22, 16]. For rigorous evaluation, we apply several attacks at rates and measure the maximum elicited forget performance of all attacks. Reference ys. Oracle Models. In our framework, the reference model trained that both desired and undesired (e.g., a pretrained LLM). We also define an oracle one trained on the reference model\u2019s pretraining dataset with all content directly related to the undesirable capability perfectly filtered out. The oracle model often represents a gold standard solution to the robust unlearning problem, simulating perfect capability removal by never learning capabilities in the place [48, 84] definition used above differs from machine used in privacy and fairness applications [28, 86]. Machine in privacy and applications the retain and forget sets partition the reference training data, and aims to produce an model equivalent to training solely on the retain set In contrast, the capabilities-focused definition uses retain and forget sets as curated datasets reflecting desired and undesired capabilities, Which need not correspond dircetly to subsets of the training data Despite this shift, similar methods are often applied across both settings; however, these methods do not achieve robustness 32]. Oracle Matching Does Not Guarantee Robust Unlearning Unlearning methods often finetune the outputs of the reference model. For example, Gradient Ascent combined with Gradient Descent maximizes loss on the forget set and minimizes it on the retain set [48]. The aspirational goal of such procedures is oracle matching: attaining outputs from those of a model that never encountered the undesired data, In this section, we demonstrate the limitations of this aspiration empirically. We finetune a reference to match an oracle model\u2019s outputs on both the retain and forget sets to achieve effectively perfect behavioral agreement. Yet, when both models are subsequently finetuned on the forget set, the oracle-matched reference model relearns significantly faster than the oracle This shows that optimizing for behavioral outputs insufficient for robust unlearning. Datasets and We conduct experiments in a language setting and an arithmetic setting, defined below. For each setting, we train model with both retain and forget capabilities, an oracle model with only retain capabilities by construction, More details on how these datasets and reference models were generated can be found in Appendix A. Language. The retain set is English documents from FineWebEdu [56]; the forget set is Korean documents from FineWeb2 the training data is the union of these two sets. The 100M parameter model based on the Gemma 2 architecture (see Table 1 for detai pretrained on 2B tokens, 1B from retain data and 1B from forget; the oracle model is the same 100M model on IB tokens of the retain set. The retain and forget are cross-entropy loss on held-out portions of the retain and forget sets, each containing 500K tokens. Arithmetic. The retain set is statements (equations and word problems) from a synthetic arithmetic dataset described in Appendix A; the forget multiplication and division from the same training dataset union of these with (English) from FineWebEdu [56]; each containing 500K arithmetic questions for training. The reference \u2018model for 1000 steps on the training data; the oracle for 1000 steps on the training data with the forget set replaced with additional English Fineweb data. The retain and forget evaluations measure accuracy on addition/subtraction and multiplication/division held-out portions of the retain and forget",
      "selected_image": "Output\\Distillation_Robustifies_Unlearning\\Selected\\thumb-0155.png"
    },
    {
      "type": "slide",
      "start_time": "0.0:5.0:39",
      "start_index": 171,
      "offset": 339.88581818181814,
      "source": "Output\\Distillation_Robustifies_Unlearning\\thumbnails\\hi\\thumb-0172.png",
      "text_ocr": "+ is a KL divergence term that pushes the output toward uniformity on forget examples, and is the standard loss on retain examples. Rather than directly opposing associations as in GradDiff, MaxEnt makes the model outputs maximally uninformative on undesired Unlearning Method C: Representation Misdirection for Unlearning (RMU) operates at the representation level rather than the output level. It minimizes \u00a3(0) mean squared error (MSE) loss to push the internal activations at specific layers toward a random direction for forget examples, and applies MSE to maintain similarity to the original model\u2019s activations for retain examples, Figure 3: Comparing unlearning methods. Unleaming trends across hyperparameters for our setup, where we select configurations that maximize retain performance while minimizing forget performance for distillation (see Figures 4 and 5). (d-f) trends in arithmetic. Experimental Setup. We investigate whether can enhance the robustness of methods against attacks. Our experimental protocol consists of two phases: (i) methods MaxEnt, or RMU) to a pretrained model (as shown in Figure 3), and this suppressed model into a randomly initialized model of identical architecture using forward We refer to this method as We observe relearning speed when models are subjected to relearning attacks, comparing models, models, and oracle baselines. We defer the discussion of other relevant experimental details to Appendix B. Result: Distillation makes unlearning more resilient to Figure 4 presents the relearning for our experiments across both language and arithmetic domains. The plots track how quickly model reacquires the forget capability when subjected to relearning attacks. We observe that that underwent unlearning followed by (Unlearn-and-Distill) significantly more resistant to relearning compared to counterparts Notably, in some cases, the distilled trajectories closely approximate those of the gold standard (Data Filtering). This robustness improvement holds whether we measure average performance rates (Figure 10) or worst-case adversarial performance (Figure 4). While RMU combined with distillation shows less impressive results in the arithmetic domain in Figure 4 offers marked improvement over the unleaming-only model. We observe that such can occur when the initial retain-forget trade-off established by the method is less favorable. In Figure 3, RMU achieved only 62% retain and 6.8% forget accuracy, compared to superior 80% and 1.3% (Appendix B.1). suggests that can depend on the quality of the teacher Why This Matters. To contextualize the experiment, we began with custom-pretrained language models containing dual capabilities. For the language setup, approximately 50% of the pretraining data contributed to desired (retain) capabilities and 50% to undesired (forget) Standard methods use minimal labeled data (less than 0.01% of pretraining data) to suppress \u2018undesired almost perfectly. However, this shallow suppression is vulnerable to attacks An altemative solution would be data filtering with full but this approach requires labeling all pretraining data, which is often infeasible at scale. bridges this gap by transferring the suppressed behavior to parameter space using unlabeled pretraining for distillation, achieving substantial robustness improvements without the extensive labeling requirements of results provide empirical evidence validating claims of prior work about the challenges of robust Some have argued that quick relearning of supposedly unlearned capabilities stems from latent information preserved in the parameter space [88, 55, 64] due to properties like",
      "selected_image": "Output\\Distillation_Robustifies_Unlearning\\Selected\\thumb-0172.png"
    },
    {
      "type": "slide",
      "start_time": "0.0:6.0:15",
      "start_index": 189,
      "offset": 375.66327272727267,
      "source": "Output\\Distillation_Robustifies_Unlearning\\thumbnails\\hi\\thumb-0190.png",
      "text_ocr": "Figure 8: UNDO makes MaxEnt and RMU more resilient to relearning Relearning trends averaged across 4 seeds for WMDP-Bio (a, b) and WMDP-Cyber (c, d). The performance was measured on WMDP benchmark. Best adversaries are bolded, Figure 8 shows that UNDO consistently improves relearning resilience tions. These results have immediate practical implications. When model dis for efficiency, incorporating unlearning before provides robustness benefits at minimal additional cost. This approach integrates capability removal into existing distillation workflows, providing a practical path to robust Limitations. In our WMDP experiments on a pretrained Gemma-2-2B model, the UNDO achieves lower on MMLU on average. UNDO on the other performance in this setting, as be seen in Appendix Figure This contrasts with the prior arithmetic and language setting results, where we see clear gains on the Pareto frontier in Figure 7. We hypothesize this difference from the fact that we undertrain in the WMDP setting, distilling on only 0.015% of the size of the original pretraining corpus compared to 20-35% in the other settings. This is further discussed in Appendix D.2.7). We expect that AI companies applying our method would not encounter the same challenges, and would have sufficient access to compute, pretraining data, and even proprietary methods to further accelerate it. Distillation is considerably more costly than finetuning-based methods, but we expect this cost to be when robust capability removal is Work Machine aims to remove the influence of training examples from a trained model. This term was by Cao and Yang [10], which explored adjusting a trained outputs to mimic retraining without the data to be forgotten. Machine has been research focus for differential privacy, implementing the \u201cright to be forgotten\u201d through frameworks that ensure removal of specific datapoints [37, 3, 27, 44, 74]. field into exact [8, 23, 14, 50] and approximate untearning [26, 63, 41, 76]. Exact targets that the unlearned parameters exactly match those of a model retrained from scratch without the be forgotten while approximate relaxes this constraint to only output distributions For LLMs, exact becomes impractical due to scale and non-convexity leading recent studies to frame as an optimization problem that approximates the behavior of the retrained model 48, 34, 43, 35, 84, 78]. LLM unlearning typically involves finetuning using like maximizing prediction loss on forget sets 85, 5, 88] or aligning token distributions with target distributions using KL divergence [38, 79, 12, 77]. Recent studies demonstrate that existing LLM unlearning methods achieve behavioral suppression rather than true (30, 73], remaining vulnerable to finetuning attacks that quickly recover suppressed [31, 47, 18]. This motivates our student-teacher approach [15] for robust There is a disconnect between what a model does and what it could do. Our findings illuminate this gap in two by showing that even models retain latent capabilities vulnerable to adversarial elicitation; and second, by demonstrating that distillation discards these while preserving desired behavior. This a standard practice in LLM development, into a security measure, offering a practical path to robust capability removal.",
      "selected_image": "Output\\Distillation_Robustifies_Unlearning\\Selected\\thumb-0190.png"
    },
    {
      "type": "slide",
      "start_time": "0.0:1.0:53",
      "start_index": 57,
      "offset": 113.29527272727272,
      "source": "Output\\Distillation_Robustifies_Unlearning\\thumbnails\\hi\\thumb-0058.png",
      "text_ocr": "Figure 2: Matching oracle behavior doesn\u2019t guarantee robust (a) KL divergence during distillation shows behavioral alignment with Oracle Teacher. (b-c) Despite this alignment, reference models matched to the oracle exhibit rapid of undesired capabilities when finetuned on the forget set, compared to the randomly initialized model matched to the oracle (Student and the oracle itself (Oracle \u2018Teacher). Results highlight that an ideal behavior on the surface is insufficient for ensuring robustness against relearning. Matching Training. We match the reference model\u2019s outputs with those of the oracle model using the Kullback-Leibler Divergence (KL Divergence) as the loss For comparison, we also match a randomly initialized model to the oracle model in the same way. The hyperparameters the distillation setup can be found in Table 3 and the loss curves are shown in Figure 2 (a), where use the label Student (Reference) or Student (Random) depending on whether the student in the setup is the reference model or the randomly initialized model, respectively. In both settings, the distillation loss tends to zero. Relearning. After we apply attacks to all three models, Student (Reference), Student (Random), and Oracle Teacher, for 500 steps on data from the The hyperparameters for are in Table 4. We show the corresponding leaming and accuracy curves for multiple rates in Figure 2 (b) and bolded lines correspond to the worst-case adversary (maximum for that number of training steps. In both the arithmetic and language settings, Student (Reference) quickly learns the forget distribution compared to the Student (Random) and Oracle Teacher models, which are more robust. This pattern holds both for the worst-case adversary and across all earning in Figure 2, the oracle\u2019s outputs into a initialized student (Student results in a model whose relearning speed on the forget distribution closely matches that of the oracle itself. Given that existing machine unlearning methods provide practical means of approximating oracle outputs, an important next question is whether this robustness to remains after replacing oracle outputs with approximations. In the next section, we demonstrate that they do. Distillation Robustifies Unlearning \u2018The previous section showed that training the reference model to match oracle insufficient for robust However, training a randomly initialized model to match the same oracle \u2018outputs does produce effect. Here, we investigate whether this robustness persists replacing the oracle with approximations from standard unlearning methods Unlearning Methods. We experiment with three unlearning methods from the literature: Gradient Difference [48], Maximizing Entropy (MaxEnt) and Representation for Unlearning (RMU) [40]. These methods represent major paradigms in LLM unlearning: manipulation, output distribution matching, and interventions. Unlearning Method A: Gradient Difference (GradDitf) applies opposing gradient updates to forget and retain data. Given a model parameterized by 0, minimizes the objective \u00a3(0) + both loss terms are cross-entropy losses. The gradient aim to maximize loss on forget examples (pushing probability mass away from undesired outputs) while minimizing loss on retain examples (preserving desired model behavior). Unlearning Method B: Maximizing Entropy (MaxEnt) increases uncertainty in the \u2018outputs on forget data while preserving performance on retain data. MaxEnt minimizes",
      "selected_image": "Output\\Distillation_Robustifies_Unlearning\\Selected\\thumb-0058.png"
    },
    {
      "type": "slide",
      "start_time": "0.0:3.0:42",
      "start_index": 112,
      "offset": 222.6152727272727,
      "source": "Output\\Distillation_Robustifies_Unlearning\\thumbnails\\hi\\thumb-0113.png",
      "text_ocr": "6 Comparisons with Other Unlearning Methods now compare UNDO against existing robust methods to evaluate their effectiveness across two key performance and resistance to relearning. The ideal unlearning method preserve high performance on desired tasks while remaining resistant to adversarial attempts to recover forget through finetuning. We first compare diverse methods \u2018our language and arithmetic tasks in Figure 7, subjecting models to stronger adversarial attacks. We compare against several approaches that address the robust unlearning problem using different to resist relearning attacks. Sharpness-Aware Minimization (SAM) optimizes for flat loss regions where small parameter perturbations maintain similar outputs, making the model less sensitive to fine-tuning. Representation Noising [59] combines with noise at the representation level, creating interference in the mode!\u2019s internal representations of forget capabilities. UNDIAL [19] also uses for unlearning but differs from our approach by not applying global parameter corruption techniques like noising. For UNDO, we fix a to 0.6 and 3 0.1, and distill from a model with varying retain thresholds? to obtain different points along the retain-forget trade-off curve. Figure 5 suggests that higher would yield robustness, while lower values would yield less. We discuss other relevant experimental details in Appendix * Figure 7: Comparing unlearning methods across different adversarial strengths. (a, performance after but before adversarial attacks. (b, forget performance after moderate relearning (40 Performance after extensive (500 As Figure 7 shows, while more compute-efficient methods MaxEnt) achieve initial retain-forget before relearning attacks, they rapidly degrade under adversarial pressure. In general, we observe that methods designed for robustness (SAM, RepNoise) also show significant performance deterioration when subjected to stronger relearning attacks. In contrast, UNDO with MaxEnt maintains more robust performance across all explored attack strengths. This creates a new Pareto frontier that approaches the gold standard of with full but requires less compute and data labeling, Moving beyond our synthetic language and arithmetic tasks, we also test our \u2018on the more realistic Weapons of Mass Destruction Proxy (WMDP) benchmark [40]. For this setup, we use Gemma-2-2B [71] rather than custom pretrained models. Following our established methodology, we first apply standard using either MaxEnt or RMU, then apply UNDO with = 0.25 and = 0. We compare this approach against the other robust We evaluate robustness against seven diverse adversarial scenarios involving different data mixtures, formats, model perturbations, and rate variations (See Appendix (0.05, 0.09, 0.13, 0.17, 0.25, 0.29, 0.33) language and arithmetic",
      "selected_image": "Output\\Distillation_Robustifies_Unlearning\\Selected\\thumb-0113.png"
    }
  ],
  "crop": [
    0,
    0,
    832,
    1080
  ]
}