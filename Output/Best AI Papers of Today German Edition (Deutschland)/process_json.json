{
  "combine_json": {
    "slides": [
      {
        "type": "slide",
        "start_time": "0.0:0.0:00",
        "start_index": 0,
        "offset": 0.0,
        "source": "Output\\Best AI Papers of Today German Edition (Deutschland)\\thumbnails\\hi\\thumb-0001.png",
        "Video_Content": "Al RESEARCH GERMAN EDITION May 19, 2025",
        "Audio_Content": [
          "Hello, come Unity.",
          "So great that you are back.",
          "Today, today a very special edition because we have today AI research at Germany edition",
          "and just to make it clear, this is a snapshot.",
          "A snapshot of time for today.",
          "So all everything that was published yesterday on May 19th, 2025.",
          "I bought some more than 1,482 AI research paper published or preprint or whatever.",
          "On the internet, I had applied my filters and I read more than 400 plus titles and scientific",
          "abstract from topics I found interesting in my area of research and I noticed something.",
          "There is today an amazing amount of new AI research publications from Germany and I think",
          "this is great that in Europe AI research is really now massive approaching a publication",
          "preprint.",
          "Whatever.",
          "So let's have a look.",
          "I feel in the US if you are in China, if you are in Southeast Asia, if you are in Australia,"
        ]
      },
      {
        "type": "slide",
        "start_time": "0.0:0.0:59",
        "start_index": 30,
        "offset": 59.639023668639055,
        "source": "Output\\Best AI Papers of Today German Edition (Deutschland)\\thumbnails\\hi\\thumb-0031.png",
        "Video_Content": "\\CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process classical mechanics, we present a novel approach establishing a CoT- Jinhe Haokun Chen! Mang Xun Hinrich Ludwig Maximilian University of Munich Munich Research Center, Huawei Technologies 3 School of Computer Science, Wuhan University Munich Center for Machine Learning 19 May 2025 arXiv Abstract Large Reasoning Models (LRMs) significantly improve the reasoning ability of Large Language Models (LLMs) by learning to reason, exhibiting the promising performance in solving complex tasks. LRMs solve a task that require complex reasoning by explicitly generating a chain-of-thought (CoT) reasoning trajectory before concluding an answer. Nevertheless, judging the quality of such an output answer is not easy because only considering the correctness of the answer is not enough and the soundness of the reasoning trajectory part matters as well. Logically, if the soundness of the reasoning part is poor, even if the answer is correct, the confidence of the derived answer should be low. Existing methods did consider a joint assessment with taking into account the reasoning part, however, their precision is unsatisfactory as the causal relationship of the reasoning to the concluded answer still cannot properly reflected. In this paper, inspired by classical mechanics, we present a novel approach towards establishing a CoT- Kinetics energy equation for the reasoning process. Specifically, our CoT-Kinetics energy equation formulates the token state transformation process regulated by LRM internal transformer layers, as like a particle kinetics dynamics governed mechanical field. Our CoT-Kinetics energy assigns a scalar score to evaluate particularly the soundness of the reasoning phase, telling how confident the derived answer could be based on the evaluated reasoning. As such, the LRM\u2019s overall output quality can be measured with finer granularity, rather than a coarse judgment (e.g., correct or incorrect) anymore. We comprehensively evaluated the fidelity of the CoT-Kinetics energy modeling. Results justify that our CoT-Kinetics energy score indeed logically reflects the causal relationship of the reasoning part and the derived final answer, outperforming existing baselines in terms of assessment metrics of AUROC, AUPR and across seven open-source LRMs and six nized benchmarks. Beyond that, our work shows a potential to assist Kinetics energy equation reasoning process. Specifically, our CoT- Kinetics energy equation the token state transformation process egulated by LRM internal ransformer layers, as like a ticle kinetics dynamics overned in a mechanical field.",
        "Audio_Content": [
          "somewhere, if you are in Antarctica.",
          "I just want to give you a feeling that in Germany in Europe we have now more and more also",
          "AI research and I think it's beautiful.",
          "It's fascinating.",
          "So I just want to give you a feeling here.",
          "First one is here, a chain of sword kinetics, a theoretical model assessing here, the",
          "large reasoning model, the reasoning process and they establish the kinetics energy",
          "equation for the reasoning process.",
          "It's just beautiful.",
          "So you have a particle kinetics dynamic governed by mechanical field and you have this",
          "here, the Ludwig Maximilian University of Munich, the Munich Research Center before",
          "a biotechnology, Munich Center of Machine Learning and the School of Computer Science in Wuhan,",
          "Rio-Han University.",
          "Great.",
          "Also we have here by Infa in Leipzig and Chemnitz University of Technology in Leipzig,"
        ]
      },
      {
        "type": "slide",
        "start_time": "0.0:1.0:51",
        "start_index": 56,
        "offset": 111.3261775147929,
        "source": "Output\\Best AI Papers of Today German Edition (Deutschland)\\thumbnails\\hi\\thumb-0057.png",
        "Video_Content": "LLM-KG-Bench 3.0: A Compass for Semantic Technology Capabilities in the Ocean of LLMs n Lars-Peter Which LLM is Brei* Claus Michael th b t offerin gthe bes ' InfAl, Leipzig, Germany, 2 Chemnitz Ur of Technology, Germany, https: //tu-chemnitz.de/ bili h N Leipzig University, Germany, Ca pa ities t > DFKI, Kaiserslautern, Germany, https: field of Semantic Web and Abstract. Current Large Language Models (LLMs) can assist devel- oping program code beside many other things, but can they support K | G working with Knowledge Graphs (KGs) as well? Which LLM is offering nowle ge p the best capabilities in the field of Semantic Web and Knowledge Graph E KGE)? Engineering (KGE)? Is this possible to determine without checking many g g G ) answers manually? The LLM-KG-Bench framework in Version 3.0 is de- ngineerin signed to answer these questions. It consists of an extensible set of tasks for automated evaluation of LLM answers and covers different aspects of > working with semantic technologies. In this paper the LLM-KG-Bench framework is presented in Version 3 along with a dataset of prompts, answers and evaluations generated with it and several state-of-the-art LLMs. Significant enhancements have been made to the framework since its initial release, including an updated task API that offers greater flexibility in handling evaluation tasks. vised tasks, and extended support for various open models through the a library, among other improvements. A comprehensive dataset has been generated using more than 30 contemporary open and proprietary N LLMs, enabling the creation of exemplary model cards that demonstrate the models\u2019 capabilities in working with RDF and SPARQL, as well as comparing their performance on Turtle and JSON-LD RDF serialization tasks Resource type: evaluation benchmark framework License: MPL 2.0 5 URL: Keywords: LLM RDF - SPARQL Knowledge Graph LLM Evalu- ation Q",
        "Audio_Content": [
          "University and Kaiser's Louten.",
          "Hello Kaiser's Louten.",
          "Yes, we have a new benchmark in this is about a large language model and a knowledge",
          "of graph benchmark and they say which LLM is offering here the basketball abilities",
          "and the field of semantic web and knowledge graph engineering.",
          "What a beautiful study.",
          "We go on and we have here Merckty, the Spence, Argy, of course German Merckty, the Spence.",
          "And they have here the development and the evolution of a cognitive AI memory framework.",
          "So they go here with agent, the memory framework for the long-term interaction with intelligent",
          "agents, of course, that developed your three-module, the memory control as a central decision",
          "unit, the memory retrieval, which filter see the relevant information from the interaction data",
          "and the post-thinking, which maintains here the memory storage and clears the memory storage.",
          "But we go on and we have here the Technical University of Munich and Technical University of Munich"
        ]
      },
      {
        "type": "slide",
        "start_time": "0.0:2.0:46",
        "start_index": 84,
        "offset": 166.98926627218935,
        "source": "Output\\Best AI Papers of Today German Edition (Deutschland)\\thumbnails\\hi\\thumb-0085.png",
        "Video_Content": "we introduce a novel framework for evaluating Interactional fairness - encompassing Inter-personal fairness (IF) and Informational fairness (InfF) - in LLM-based multi-agent systems (LLM- MAS). 5 202 [cs.AI] 2 Interactional Fairness in LLM Multi-Agent Systems: An Evaluation Framework Ruta Binkyte CISPA Helmholtz Center for Information Security Germany Abstract As large language models (LLMs) are increasingly used in multi-agent systems, questions of fairness should extend be- yond resource distribution and procedural design to include the fairness of how agents communicate. Drawing from or- ganizational psychology, we introduce a novel framework for evaluating Interactional fairness \u2014 encompassing Inter- personal fairness (IF) and Informational fairness (InfF) \u2014in LLM-based multi-agent systems (LLM-MAS). We extend the theoretical grounding of Interactional Fairness to non- sentient agents, reframing fairness as a socially interpretable signal rather than a subjective experience. We then adapt es- tablished tools from organizational justice research, including Colquitt\u2019s Organizational Justice Scale and the Critical Inci- dent Technique, to measure fairness as a behavioral property of agent interaction. We validate our framework through a pilot study using controlled simulations of a resource nego- tiation task. We systematically manipulate tone, explanation quality, outcome inequality, and task framing (collaborative vs. competitive) to assess how IF influences agent behavior. show that tone and justification quality tance decisions even when objective outcomes are held constant. In addition, the influence of IF vs. InfF varies with context. This work lays the foundation for fair- ness auditing and norm-sensitive alignment in LLM-MAS. Introduction Large Language Models (LLMs) are increasingly deployed in multi-agent systems (MAS), enabling agents to interact, negotiate, and coordinate through expressive natural lan- guage. These agents are no longer confined to mechani- cal task execution; they now engage in communicative acts that resemble human-like reasoning and social behavior. As LLM-driven MAS grow in scope and are increasingly used 2021; Gajane et al. 2022), resource allocation (Zhang and Shah 2014; Li and Ma 2023; Bu et al. 2023; Amanatidis et al. 2023), and social choice theory (La Malfa et al. 2024). However, this focus overlooks a third dimension: the Inter- actional fairness, which evaluates how decisions are deliv- ered, justified, and socially enacted. As agents begin to use nuanced language in high-stakes settings, fairness can no longer be assessed by outcomes and procedures alone. Although prior MAS research has explored related con- cepts, such as politeness in negotiation (De Jong et al. 2005) and the influence of trust and reciprocity on fairness judg- ments (Zhang 2008), these studies focused on non-LLM agents with limited linguistic and social capabilities. More recent work has shown that LLMs exhibit sensitivity to tone, politeness, and social roles (Park et al. 2022; Ganguli et al. 2022; Park et al. 2023), but these findings come largely from single-agent scenarios or open-ended social simula- tions, rather than structured multi-agent systems with strate- gic goals. In organizational psychology, the communicative dimen- sion of fairness is captured by Interactional fairness, which distinguishes between: Interpersonal fairness (IF): Respectful treatment and dignified tone during communication; Informational fairness (InfF): Clarity, honesty, and ad- equacy of explanations for decisions. Organizational psychology research has shown that Inter- actional fairness is an important factor alongside Distribu- tional and Procedural fairness and can increase cooperation and reduce the propensity to conflict or deception in hu- man teams (Greenberg and Cropanzano 1993; Zhang, Yin, and Wu 2024). We argue that these",
        "Audio_Content": [
          "and they have here a language model that walk to talk for formal fairness certificates.",
          "And this is interesting because they want to have here on the internet or news wherever you are",
          "a toxicity detection.",
          "Offering here kind of a formal guarantee that adversarily manipulated toxic input out consistently",
          "detected and appropriately censored.",
          "Interesting field of study.",
          "Then we have here Helmord Center for Information Security and Germany, the Inter-Axial Fairness.",
          "So you see this is now seems to be a real focal point here on multi-agents system and",
          "evaluation framework and they introduce in your framework for evaluating Inter-Axial Fairness,",
          "encompassing here the Inter-Portional Fairness and the Information-Alfairness in LIM-based",
          "multi-agents system.",
          "Why do an interesting topic?",
          "I've not seen this with fairness in the American literature, for example."
        ]
      },
      {
        "type": "slide",
        "start_time": "0.0:3.0:44",
        "start_index": 113,
        "offset": 224.6403224852071,
        "source": "Output\\Best AI Papers of Today German Edition (Deutschland)\\thumbnails\\hi\\thumb-0113.png",
        "Video_Content": "2505.13116v1 [cs.LG] 19 May 2025 arXiv Continuous Fair SMOTE Fairness-Aware Stream Learning from Imbalanced Data Kathrin Lammers Valerie Vaquet Barbara Hammer Machine Learning Group Bielefeld University, Bielefeld - Germany Abstract machine learning is increasingly applied in an online fashion to deal with evolving data streams, the fairness of these algorithms is a matter of growing ethical and legal concern. In many use cases, class imbalance in the data also needs to be dealt with to ensure predictive performance. Current fairness-aware stream learners typically attempt to solve these issues through in- or post-processing by focusing on optimizing one specific discrimination metric, addressing class imbalance in a separate processing step. While C-SMOTE is a highly effective pre-processing approach to mitigate class imbalance, as a side effect of this method, algorithmic bias is often introduced. \u2018Therefore, we propose CFSMOTE - a fairness-aware, continuous SMOTE as a pre-processing approach to simultaneously address the class imbalance and fairness concerns by employing situation testing and bal- ancing fairness-relevant groups during oversampling. Unlike other fairness- aware stream learners, CFSMOTE is not optimizing for only one spe- cific fairness metric, therefore avoiding potentially problematic trade-offs. Our experiments show significant improvement on several common group fairness metrics in comparison to vanilla C-SMOTE while maintaining competitive performance, also in comparison to other fairness-aware algo- rithms. Introduction We increasingly rely on machine learning algorithms for many different aspects of life, such as communications, healthcare, infrastructure, education, and fi- nance. The question of whether these algorithms are fair is of ever-increasing importance to society Legislation, such as the EU AI Act, demands that algorithmic decision-making is free from discrimination and unfair biases and a growing body of work has emerged in the area of fair machine learning in recent years stream learning has been an area of growing interest world where real-time applications in areas such as e-commerce or stock trading operate not on static data but on data streams, where individual samples arrive in the scope of the BMBF project KI Akademie OWL under grant agreement No is gratefully acknowledged.",
        "Audio_Content": [
          "Well here we have Tio Darmstadt.",
          "Hey, Darmstadt.",
          "Oh my goodness, 20 years ago in Darmstadt.",
          "They are computer science department and they're Hessian Center for the Vittal Intelligence,",
          "Darmstadt and General Research Center for the Vittal Intelligence, Darmstadt, beautiful.",
          "And they say, hey, how can we enable reinforcement learning agents to have similar human",
          "priors here allowing here the agent to learn with you training interaction, reinforcement learning",
          "and they are developed.",
          "Darmstadt of course here Tio, minute-care, you immediately see what we are talking about.",
          "In hand manipulation via scoring with a reinforcement learning critique model,",
          "interesting for robotics or we go here with Heidelberg University, Mannheim, Germany.",
          "And they have here a zero-shot inference of dynamic system,",
          "preserving the long-term statistics.",
          "So this is an interesting interplay here, with a mixture of expert architecture,",
          "pre-trained for a dynamical system reconstruction.",
          "So if you are here into timelines, this is it.",
          "Plus, a new research here by Bielefeld University,",
          "Hello Bielefeld.",
          "And we have here the question whether the algorithms, the machine learning algorithms are",
          "in an ever-increasing importance to our society.",
          "So interesting referencing here to the European Union AI Act.",
          "So there's a lot of research, how to implement this and what about fairness?"
        ]
      },
      {
        "type": "slide",
        "start_time": "0.0:5.0:12",
        "start_index": 157,
        "offset": 312.1108905325444,
        "source": "Output\\Best AI Papers of Today German Edition (Deutschland)\\thumbnails\\hi\\thumb-0158.png",
        "Video_Content": "ModernGBERT: German-only 1B Encoder Model Trained from Scratch Anton and Julia and Jan Pfister* Fotis and Andreas Computer Philology and History of Contemporary German Data Science* CAIDAS Center for Artificial Intelligence and Data Science JMU Abstract Despite the prominence of decoder-only lan- guage models, encoders remain crucial for resource-constrained applications. We intro- duce ModernGBERT (134M, 1B), a fully transparent family of German encoder mod- els trained from scratch, incorporating archi- tectural innovations from ModernBERT. To evaluate the practical trade-offs of training en- coders from scratch, we also present LLiMm- lein2Vec (120M, 1B, 7B), a family of encoders derived from German decoder-only models via LLM2Vec. We benchmark all models on natural language understanding, text embed- ding, and long-context reasoning tasks, abling a controlled comparison between de icated encoders and converted decoders. Our results show that ModernGBERT 1B outper- forms prior state-of-the-art German encoders as well as encoders adapted via LLM2Vec, with regard to performance and parameter-efficiency All models, training data, checkpoints and code are publicly advancing the Ger- man NLP ecosystem with transparent, high- performance encoder models. Despite the recent dominance of decoder-only large language models (LLMs), parameter-efficient en- coder models remain crucial for language tech- nology, particularly for local deployments such as retrieval-augmented generation (RAG). Their bidirectional attention confers strong understand- ing capabilities with lower resource requirements, h: attractive far 0.78 074 0.70 Parameter Figure Performance on SuperGLEBer benchmark. markers: encoders, markers: decoders. Dashed arrows: conversion gains. Models of the same family are colored in the same color. limited training data (163 GB). More recently, Mod- emBERT (Warner et al., 2024) introduced several architectural improvements for English encoders, including enhanced relative positional embeddings and efficient attention patterns enabling long con- text processing. Building on this progress and in- spired by the success of (Pfister et al., We introduce ModernGBERT (134M, 1B), a fully transparent family of German encoder models trained from scratch incorporating architectural ModernBERT.",
        "Audio_Content": [
          "We go on, we have here Bielefeld University again, beautiful.",
          "May 20, 25, 25, 2, day as you see and they go about building and utilizing dynamic",
          "partner models for an adaptive explanation generation.",
          "And you see we're working here with our colleagues in the social cognitive systems,",
          "constructing explainability.",
          "This is interesting. So you see, sometimes the focal point on a particular day,",
          "are really not where we have the US American focal points or the development points where"
        ]
      },
      {
        "type": "slide",
        "start_time": "0.0:5.0:47",
        "start_index": 175,
        "offset": 347.89430473372784,
        "source": "Output\\Best AI Papers of Today German Edition (Deutschland)\\thumbnails\\hi\\thumb-0176.png",
        "Video_Content": "We introduce a novel continuo uses an epsilon- neighborhood- and learns to apply the convolution operator to adaptive and constrained arXiv:2505.12944v1 [cs.LG] 19 May 2025 CALM-PDE: Continuous and Adaptive Convolutions for Latent Space Modeling of Time-dependent PDEs Jan Hagnberger* Daniel Mathias Machine Learning and Simulation Lab, Institute for Artificial Intelligence, University of Stuttgart Max Planck Research School for Intelligent Systems (IMPRS-IS) Center for Simulation Science (SimTech) Abstract Solving time-dependent Partial Differential Equations (PDEs) using a densely discretized spatial domain is a fundamental problem in various scientific and engi- neering disciplines, including modeling climate phenomena and fluid dynamics. However, performing these computations directly in the physical space often incurs significant computational costs. To address this issue, several neural surrogate models have been developed that operate in a compressed latent space to solve the PDE. While these approaches reduce computational complexity, they often use Transformer-based attention mechanisms to handle irregularly sampled domains, resulting in increased memory consumption. In contrast, convolutional neural networks allow memory-efficient encoding and decoding but are limited to regular discretizations. Motivated by these considerations, we propose CALM-PDE, a model class that efficiently solves arbitrarily discretized PDEs in a compressed latent space. We introduce a novel continuous convolution-based encoder-decoder architecture that uses an epsilon-neighborhood-constrained kernel and learns to apply the convolution operator to adaptive and optimized query points. We demon- strate the effectiveness of CALM-PDE on a diverse set of PDEs with both regularly and irregularly sampled spatial domains. CALM-PDE is competitive with or out- performs existing baseline methods while offering significant improvements in memory and inference time efficiency compared to Transformer-based methods. Many scientific problems, such as climate modeling and fluid mechanics, rely on simulating physical systems, often involving solving spatio-temporal Partial Differential Equations (PDEs). In recent years, Machine Learning (ML) models have been successfully used to approximate the solution of PDEs (Lu , 2019; tal., 2022; Cher offering",
        "Audio_Content": [
          "China has here. It's focused interesting.",
          "Talking about specific German focal points, what about a modern G-Burt,",
          "a German only one billion free-trained,",
          "a parameter encoder model trained from scratch.",
          "This is great icing every nation should have its own language more in its own native language.",
          "Why if you have multiple languages, at least one of your dominant languages,",
          "I don't think that everything should be done in English.",
          "So great if you have a diversification, if you go in your",
          "modern tongue in your national primary language and you develop AI in your",
          "for your society, in your language modern bird.",
          "Yeah, they don't go with the classical bird system that have in this",
          "in the innovation of the modern bird system. And if you want, yes, of course I have a video on modern",
          "bird. Then we have here where we are a max plank, of course, Stuttgart Center for"
        ]
      },
      {
        "type": "slide",
        "start_time": "0.0:6.0:41",
        "start_index": 202,
        "offset": 401.56942603550294,
        "source": "Output\\Best AI Papers of Today German Edition (Deutschland)\\thumbnails\\hi\\thumb-0202.png",
        "Video_Content": "we perform bias clas 2505.13010v1 [cs.CL] 19 May 2025 arXiv To Bias or Not to Bias: Detecting bias in News with bias-detector Himel Ahmed Mosharafa', Georg Groh! 'Technical University of Munich (TUM), Germany, University of Rome, Italy, himel.ghosh@tum.de Abstract Media bias detection is a critical task in en- suring fair and balanced information dissem- ination, yet it remains challenging due to the subjectivity of bias and the scarcity of high- quality annotated data. In this work, we per- form sentence-level bias classification by fine- tuning a ROBERTa-based model on the expert- annotated BABE dataset. Using McNemar\u2019s test and the 5x2 cross-validation paired t-test, we show statistically significant improvements in performance when comparing our model to pre-trained DA-RoBERTa Furthermore, attention-based analy- sis shows that our model avoids common pit- falls like oversensitivity to politically charged terms and instead attends more meaningfully to contextually relevant tokens. For a com- prehensive examination of media bias, we present a pipeline that combines our model isting bias-type classifier. ts good generalization and interpretability, despite being constrained by sentence-level analysis and dataset size because of a lack of larger and more advanced bias cor- We talk about context-aware modeling, bias neutralization, and advanced bias type clas- sification as potential future directions. Our findings contribute to building more robust, ex- plainable, and socially responsible NLP sys- tems for media bias detection. Keywords: Bias Detection, Media Bias, NLP, Interpretability, Sentence Classification, transfer The brave leader saved the country. Bias-Detector Figure 1: Bias detection and Classification scheme. particular individuals and ideas as well as the re- porting of certain biased viewpoints (Morrisette et which can significantly influence public opinion, skew political and democratic discourses. For a free and fair journalism and news report- ing, it is imperative to identify the bias and eventu- ally mitigate them. This paper addresses the ever- increasing need for bias identification models and demonstrates their statistical significance compared to the other available models . Here we introduce a neural transformer-based fine-tuned bias detec- tion model trained on Bias Annotations by (BABE) dataset by (Spinde et al., 2021b) for robust bias detection available on Hugging Face. Furthermore, we present a pipeline (See Fig. 1)",
        "Audio_Content": [
          "Simulation Science, Simulation Science, Simulation of Science, okay, University of Stuttgart",
          "and they have your continuous and a collaborative convolution for latent space modeling",
          "of time dependent partial differential equations.",
          "Real nice, really interesting, you know exactly what we're talking",
          "for talking about Stuttgart. And they say, hey, we introduce a",
          "now continuous convolution-based encoder decoder architecture.",
          "So I like a T5 that we know from five years ago.",
          "They'd use a Cian-Abscyline neighborhood constraint corno and learns to apply",
          "the convolution operator to adapt if an optimized, very point.",
          "Real interested if you are into the optimization of partial differential equation",
          "and their applications."
        ]
      },
      {
        "type": "slide",
        "start_time": "0.0:7.0:27",
        "start_index": 225,
        "offset": 447.2926775147929,
        "source": "Output\\Best AI Papers of Today German Edition (Deutschland)\\thumbnails\\hi\\thumb-0226.png",
        "Video_Content": "are fused using ar document-level 2505.13036v1 [cs.CL] 19 May 2025 arXiv KIT\u2019s Offline Speech Translation and Instruction Following Submission for IWSLT 2025 Sai Maike Thai-Binh Nguyen, Seymanur Akti, Jan Niehues, Alexander Waibel Karlsruhe Institute of Technology Abstract The scope of the International Workshop on Spoken Language Translation (IWSLT) has re- cently broadened beyond traditional Speech Translation (ST) to encompass a wider array of tasks, including Speech Question Answer- ing and Summarization. This shift is partly driven by the growing capabilities of modern systems, particularly with the success of Large Language Models (LLMs). In this paper, we present the Karlsruhe Institute of Technology\u2019s submissions for the Offline ST and Instruc- tion Following (IF) tracks, where we leverage LLMs to enhance performance across all tasks. For the Offline ST track, we propose a pipeline that employs multiple automatic speech recog- nition systems, whose outputs are fused using an LLM with document-level context. This is followed by a two-step translation process, incorporating additional refinement step to im- prove translation quality. For the IF track, we develop an end-to-end model that integrates a speech encoder with an LLM to perform a wide range of instruction-following tasks. We complement it with a final document-level re- finement stage to further enhance output quality by using contextual information. Introduction This paper provides an overview of the systems A growing research trend in the field is the ap- plication of Large Language Models (LLMs) to speech processing tasks (Tang et al., 2023; Ziifle and Niehues, 2024; Chu et al., 2024b; Abouelenin et al., 2025, among others), leveraging their strong general knowledge and natural language under- standing capabilities. These strengths make LLMs particularly relevant to both the Offline ST and IF tracks. Accordingly, in our submissions, we ex- plore strategies for effectively integrating LLMs into speech processing pipelines. There are multiple approaches to leveraging LLMs in speech systems. One strategy involves incorporating LLMs as an additional step within a cascaded architecture (Koneru et al., 2024a), where they can perform task-specific refinement. This modular approach allows each component to be trained independently, benefiting from specialized data. Alternatively, LLMs can be integrated in an end-to-end fashion (Tang et al., 2023; Ziifle and Niehues, 2024; Chu et al., 2024b; Abouelenin et al., 2025), allowing for better information flow and po- tentially improving generalization to unseen tasks. Although both the Offline and IF tasks fall un- der the umbrella of speech processing, they differ significantly in nature. In the offline setting, speed and adaptability to unseen tasks are not primary concerns. In contrast, the IF task demands flexibil-",
        "Audio_Content": [
          "Real interesting is here, technical University of Munich, yes, and University of Rome.",
          "This is beautiful.",
          "Natalize two buyers on up to bias, detecting bias here in use,",
          "use publication articles, whatever TV news with a bias detector.",
          "So there's a lot of focal point here on fair reporting that every society group is really",
          "also here. Have access is here represented in a fair and open way and also they want to detect",
          "if there are any biases from any group in any particular online publication.",
          "Can we detect the bias and this is nice and they perform here.",
          "And this is so beautiful because they go back to the roots.",
          "No remember birth and sentence birth.",
          "So they tell us here in their latest publication, we perform a sentence-level bias classification",
          "by fine tuning a Roberto-based model. My goodness, last time I worked with Roberto was about three",
          "years ago on an expert annotated.",
          "Hey, babe data set. Oh, this sounds interesting. So you see, they care a lot about the",
          "media bias that might encounter you if you are looking here in the internet or whatever TV or",
          "whatever you go. They say, hey, we want to make sure everybody has here access to the real truth.",
          "Here we go on in a thing. This is for the last one, Karlsruhe Institute of Technology,",
          "get beautiful and they go here in this particular publication.",
          "Please, this is only a snapshot of one day.",
          "And they are more than one thousand paper that AI filtered out because it's not in my area of expertise.",
          "And they say, hey, we here here in doing the research on a pipeline that employs multiple",
          "automatic speech recognition systems whose outputs that then fuse with a large language model",
          "with document-level context. And this is followed up by two step translation process.",
          "In corporing, now additional refinements step to improve the translation quality,",
          "but simply providing the additional context to your two-distranslation exercise.",
          "Really interesting, kid, one of the beautiful institutions in Germany. And I think this is it."
        ]
      },
      {
        "type": "slide",
        "start_time": "0.0:9.0:38",
        "start_index": 291,
        "offset": 578.4985295857988,
        "source": "Output\\Best AI Papers of Today German Edition (Deutschland)\\thumbnails\\hi\\thumb-0292.png",
        "Video_Content": "May 19, 2025 1482 Al research paper published /pre- 400+ Titles and scientific abstracts",
        "Audio_Content": []
      },
      {
        "type": "slide",
        "start_time": "0.0:0.0:09",
        "start_index": 5,
        "offset": 9.93983727810651,
        "source": "Output\\Best AI Papers of Today German Edition (Deutschland)\\thumbnails\\hi\\thumb-0006.png",
        "Video_Content": "Al nti",
        "Audio_Content": [
          "and just to make it clear, this is a snapshot."
        ]
      },
      {
        "type": "slide",
        "start_time": "0.0:0.0:11",
        "start_index": 6,
        "offset": 11.927804733727811,
        "source": "Output\\Best AI Papers of Today German Edition (Deutschland)\\thumbnails\\hi\\thumb-0007.png",
        "Video_Content": "May 19, 2025 1482 Al research paper published /pre-print 400+ Titles and scientific abstracts",
        "Audio_Content": [
          "A snapshot of time for today.",
          "So all everything that was published yesterday on May 19th, 2025.",
          "I bought some more than 1,482 AI research paper published or preprint or whatever.",
          "On the internet, I had applied my filters and I read more than 400 plus titles and scientific",
          "abstract from topics I found interesting in my area of research and I noticed something.",
          "There is today an amazing amount of new AI research publications from Germany and I think",
          "this is great that in Europe AI research is really now massive approaching a publication",
          "preprint.",
          "Whatever.",
          "So let's have a look.",
          "I feel in the US if you are in China, if you are in Southeast Asia, if you are in Australia,",
          "somewhere, if you are in Antarctica.",
          "I just want to give you a feeling that in Germany in Europe we have now more and more also",
          "AI research and I think it's beautiful.",
          "It's fascinating.",
          "So I just want to give you a feeling here.",
          "First one is here, a chain of sword kinetics, a theoretical model assessing here, the",
          "large reasoning model, the reasoning process and they establish the kinetics energy",
          "equation for the reasoning process.",
          "It's just beautiful.",
          "So you have a particle kinetics dynamic governed by mechanical field and you have this",
          "here, the Ludwig Maximilian University of Munich, the Munich Research Center before",
          "a biotechnology, Munich Center of Machine Learning and the School of Computer Science in Wuhan,",
          "Rio-Han University.",
          "Great.",
          "Also we have here by Infa in Leipzig and Chemnitz University of Technology in Leipzig,"
        ]
      },
      {
        "type": "slide",
        "start_time": "0.0:1.0:53",
        "start_index": 57,
        "offset": 113.3141449704142,
        "source": "Output\\Best AI Papers of Today German Edition (Deutschland)\\thumbnails\\hi\\thumb-0058.png",
        "Video_Content": "ay 20 044v1 [cs.AI] 19M CAIM: Development and Evaluation of a Cognitive Al Memory Framework for Long-Term Interaction with Intelligent Agents Rebecca rebecca.westhaeusser@mercedes-benz.com Mercedes-Benz AG Germany Wolfgang Minker wolfgang.minker@uni-ulm.de Ulm University Ulm, Germany ABSTRACT Large language models (LLMs) have advanced the field of artifi- cial intelligence and are a powerful enabler for interactive systems. However, they still face challenges in long-term interac- tions that require adaptation towards the user as well as contextual knowledge and understanding of the ever-changing environment To overcome these challenges, holistic memory modeling is re- quired to efficiently retrieve and store relevant information across interaction sessions for suitable responses. Cognitive AI, which aims to simulate the human thought process in a computerized model, highlights interesting aspects, such as thoughts, memory mechanisms, and decision-making, that can contribute towards improved memory modeling for LLMs. Inspired by these cognitive Al principles, we propose our memory framework CAIM. CAIM consists of three modules: 1.) The Memory Controller as the cen- tral decision unit; 2.) the Memory Retrieval, which filters relevant data for interaction upon request; and 3.) the Post-Thinking, which maintains the memory storage. We compare CAIM against exist- ing approaches, focusing on metrics such as retrieval accuracy, response correctness, contextual coherence, and memory storage. s demonstrate that CAIM outperforms baseline frame- \u2018ross different metrics, highlighting its context-awareness and potential to improve long-term human-Al interactions. KEYWORDS Large Language Models, Long-term Memory, Cognitive Al 1 INTRODUCTION Advancements in LLMs, such as GPT-3.5 [6], are reshaping the interaction between humans and Al systems. These models allow direct and seamless communication through natural This Frederik Berenz frederik.berenz@mercedes-benz.com Mercedes-Benz AG Germany Sebastian Zepf sebastian.zepf@mercedes-benz.com Mercedes-Benz AG Germany historical information over extended periods [48], hindering their effectiveness in long-term interactions [26, 33, 36, 40, 52]. Although models such as GPT-4o [17] or Gemini [41] offer larger context windows, it is crucial to keep the input concise to ensure high per- formance, as longer inputs may reduce potentially overwhelm the model Maintaining contextual understanding is critical to ensure meaningful interactions and understand user behavior [52]. Therefore, remembering information from the past and correctly referencing it in ongoing conversations is crucial in long-term interactions [3]. Without a long-term memory (LTM), LLMs also face challenges in delivering personalized responses. Personalization stands as an important connection that bridges the gap between humans and machines, as understanding the user\u2019s preferences is fundamental to adapting to their evolving needs and ensuring user satisfaction and engagement 12, 19, 45]. Various studies emphasize the importance of personalization in building a relationship with AI tants. For example, Lee et al. indicate that adding personal- ized services enhances user engagement with an as Ligthart et al. [25] and Kirmayr et al. [19] indicate that personaliza- tion fosters closeness, maintains the user\u2019s interest over time, and adds continuity to interactions across sessions. These challenges highlight the need for a context-aware memory mechanism that effectively manages input limits and proficiently draws on infor- mation from past conversations to improve long-term interactions and personalization. In this context, methods such as reinforcement learning from hu- man feedback [37, 45] have shown promising approaches to im- prove the long-term capabilities of LLMs. Their practical application is often limited due to retraining the model, which is challenging and often impossible for models accessible only via API 36, 37]. This paper focuses on retrieval-augmented methods as a practical CAIM consists of three modules: 1.) The Memory Controller as the central decision unit; 2.) the Memory Retrieval, which filters relevant data for interaction upon request; and 3.) the Post- Thinking, which maintains the memory storage.",
        "Audio_Content": [
          "University and Kaiser's Louten.",
          "Hello Kaiser's Louten.",
          "Yes, we have a new benchmark in this is about a large language model and a knowledge",
          "of graph benchmark and they say which LLM is offering here the basketball abilities"
        ]
      },
      {
        "type": "slide",
        "start_time": "0.0:2.0:07",
        "start_index": 64,
        "offset": 127.22991715976332,
        "source": "Output\\Best AI Papers of Today German Edition (Deutschland)\\thumbnails\\hi\\thumb-0065.png",
        "Video_Content": "Language Models That Walk the Talk: A Framework for Formal Fairness Certificates extend this methodology Chen* Tobias Ladner* Technical University of Munich, University of Munich, Germany to toxi city chen. detection, Ahmed Rayen Mhadhbi Matthias Althoff Technical University of University of Munich, Germany offeri ng formal althoff@tum.de ntees Abstract th at As large language models become integral to high-stakes applications, ensuring adve rsarially their robustness and fairness is critical. Despite their success, large language models remain vulnerable to adversarial attacks, where small perturbations, such as p lated synonym substitutions, can alter model predictions, posing risks in n ar uch as gender bias mitigation, and safety-critical areas, such as toxicity detection. While formal verification has been explored for neural networks, its toxic puts application to large language models remains limited. This work presents a holistic verification framework to certify the robustness of transformer-based language are > models, with a focus on ensuring gender fairness and consistent outputs across different gender-related terms. Furthermore, we extend this methodology to toxicity detection, offering formal guarantees that adversarially manipulated toxic inputs are consistent ly consistently detected and appropriately censored, thereby ensuring the reliability of moderation systems. By formalizing robustness within the embedding space, detected and this work strengthens the reliability of language models in ethical deployment and content moderation. appropriate y Introduction censored. > As large language models become integral to academia and industry, particularly in high-stakes decision-making and safety-critical domains, ensuring their robustness, alignment, and trustworthiness is crucial [49, 67]. While their transformer-based architecture with attention mechanisms has driven widespread adoption concerns about robustness and safety persist, especially in tasks like fairness enforcement and content moderation.",
        "Audio_Content": [
          "and the field of semantic web and knowledge graph engineering.",
          "What a beautiful study.",
          "We go on and we have here Merckty, the Spence, Argy, of course German Merckty, the Spence.",
          "And they have here the development and the evolution of a cognitive AI memory framework.",
          "So they go here with agent, the memory framework for the long-term interaction with intelligent",
          "agents, of course, that developed your three-module, the memory control as a central decision",
          "unit, the memory retrieval, which filter see the relevant information from the interaction data",
          "and the post-thinking, which maintains here the memory storage and clears the memory storage.",
          "But we go on and we have here the Technical University of Munich and Technical University of Munich"
        ]
      },
      {
        "type": "slide",
        "start_time": "0.0:2.0:50",
        "start_index": 86,
        "offset": 170.96520118343196,
        "source": "Output\\Best AI Papers of Today German Edition (Deutschland)\\thumbnails\\hi\\thumb-0087.png",
        "Video_Content": "Learning from Less: Guiding Deep Reinforcement Learning with Differentiable Symbolic Planning How can we enable reinforcement learning (RL) agents to have similar allowing the agent to learn with fewer traini interac [cs.AI] 16 May 2025 arXiv Zihan Oleg Kristian Science Department, TU Darmstadt, Germany Center for Artificial Intelligence Darmstadt, Germany for Cognitive Science, TU Darmstadt, Germany 4German Research Center for Artificial Intelligence Darmstadt, Germany Abstract When tackling complex problems, humans naturally break them down into smaller, manageable subtasks and adjust their initial plans based on observations. For instance, if you want to make coffee at a friend\u2019s place, you might initially plan to grab coffee beans, go to the coffee machine, and pour them into the machine. Upon noticing that the machine is full, you would skip the initial steps and proceed directly to brewing. In stark contrast, state-of-the-art reinforcement learners, such as Proximal Policy Optimization (PPO), lack such prior knowledge and therefore require significantly more training steps to exhibit comparable adaptive behavior. Thus, a central research question arises: How can we enable reinforcement learning (RL) agents to have similar \u201chuman priors\u201d, allowing the agent to learn with training interactions? To address this challenge, we propose differentiable symbolic planner (Dylan), a novel framework that integrates symbolic planning into Reinforcement Learning. Dylan serves as a reward model that dynamically shapes rewards by leveraging human priors, guiding agents through intermediate subtasks, thus enabling more efficient exploration. Beyond reward shaping, Dylan can work as a high-level planner that composes primitive policies to generate new behaviors while avoiding common symbolic planner pitfalls such as infinite execution loops. Our experimental evaluations demonstrate that Dylan significantly improves RL agents\u2019 performance and facilitates generalization to unseen tasks. Reinforcement learning (RL) has demonstrated remarkable success across a wide range of domains,",
        "Audio_Content": [
          "and they have here a language model that walk to talk for formal fairness certificates.",
          "And this is interesting because they want to have here on the internet or news wherever you are",
          "a toxicity detection.",
          "Offering here kind of a formal guarantee that adversarily manipulated toxic input out consistently",
          "detected and appropriately censored.",
          "Interesting field of study.",
          "Then we have here Helmord Center for Information Security and Germany, the Inter-Axial Fairness.",
          "So you see this is now seems to be a real focal point here on multi-agents system and",
          "evaluation framework and they introduce in your framework for evaluating Inter-Axial Fairness,"
        ]
      },
      {
        "type": "slide",
        "start_time": "0.0:3.0:32",
        "start_index": 107,
        "offset": 212.7125177514793,
        "source": "Output\\Best AI Papers of Today German Edition (Deutschland)\\thumbnails\\hi\\thumb-0108.png",
        "Video_Content": "True Zero-Shot Inference of Dynamical Systems Preserving Long-Term Statistics \u00a9 Christoph , Daniel of Theoretical Neuroscience, Central Institute of Mental Health, Medical Faculty Mannheim, Heidelberg University, Mannheim, Germany Center for Scientific Computing (IWR), Heidelberg University, Germany 3Faculty of Physics and Astronomy, Heidelberg University, Heidelberg, Germany {christoph.hemmer, daniel.durstewitz}@zi-mannheim.de Abstract Complex, temporally evolving phenomena, from climate to brain activity, are governed by dynamical systems (DS). DS reconstruction (DSR) seeks to infer generative surrogate models of these from observed data, reproducing their long- term behavior. Existing DSR approaches require for any new system observed, lacking the zero-shot and in-context inference capabilities known from LLMs. Here we introduce DynaMix, a novel multivariate ALRNN-based mixture-of-experts architecture pre-trained for DSR, the first DSR model able to generalize zero-shot to out-of-domain DS. Just from a provided context signal, without any re-training, faithfully forecasts the long-term evolution of novel DS where existing time series (TS) foundation models, like Chronos, fail \u2014 fraction of the number of parameters and orders of magnitude faster inference DynaMix outperforms TS foundation models in terms of long-term statistics, and often also short-term forecasts, even on real-world time series, like traffic or weather data, typically used for training and evaluating TS models, not at all part of training corpus. We illustrate some of the failure modes of TS models for DSR problems, and conclude that models built on DS principles may bear a huge potential also for advancing the TS prediction field. 2505.13192v1 [cs.LG] 19 May 2025 Geometry Power Spectrum True Geometry Power Spectrum arXiv parting achieves zero-shot DSR of attractor geometry and long-term temporal properties Figure 1:",
        "Audio_Content": [
          "encompassing here the Inter-Portional Fairness and the Information-Alfairness in LIM-based",
          "multi-agents system.",
          "Why do an interesting topic?",
          "I've not seen this with fairness in the American literature, for example.",
          "Well here we have Tio Darmstadt.",
          "Hey, Darmstadt.",
          "Oh my goodness, 20 years ago in Darmstadt.",
          "They are computer science department and they're Hessian Center for the Vittal Intelligence,",
          "Darmstadt and General Research Center for the Vittal Intelligence, Darmstadt, beautiful."
        ]
      },
      {
        "type": "slide",
        "start_time": "0.0:4.0:02",
        "start_index": 122,
        "offset": 242.53202958579882,
        "source": "Output\\Best AI Papers of Today German Edition (Deutschland)\\thumbnails\\hi\\thumb-0123.png",
        "Video_Content": "SNAPE-PM: BUILDING AND UTILIZING DYNAMIC PARTNER MODELS FOR ADAPTIVE EXPLANATION GENERATION A PREPRINT Amelie S. Robrecht Christoph R. Kowalski Social Cognitive Systems Social Cognitive Systems TRR 318 | Constructing Explainability TRR 318 | Constructing Explainability Bielefeld University Bielefeld University > Germany Germany N Stefan Kopp Social Cognitive Systems TRR 318 | Constructing Explainability Bielefeld University Germany May 2 ABSTRACT Adapting to the addressee is crucial for successful explanations, yet ignificant challenges for We adopt the approach of treating explanation generation as a non-stationary decision > process, where the optimal strategy varies according to changing beliefs about the explainee and the interaction context. In this paper we address the questions of (1) how to track the interaction context and the relevant listener features in a formally defined computational partner model, and (2) how to utilize this model in the dynamically adjusted, rational decision process that determines the currently best explanation strategy. We propose a Bayesian inference-based approach to continuously update the partner model based on user feedback, and a non-stationary Markov Decision Process to adjust decision-making based on the partner model values. We evaluate an implementation of this framework with five simulated interlocutors, demonstrating its effectiveness in adapting to different partners with constant and even changing feedback behavior. The results show high adaptivity with distinct explanation strategies emerging for different partners, highlighting the potential of our approach to > improve explainable AI systems and dialogsystems in general. Keywords Explainability - Adaptivity - Partner Models Dynamic Bayesian Network - Non-stationary Decision Process",
        "Audio_Content": [
          "And they say, hey, how can we enable reinforcement learning agents to have similar human",
          "priors here allowing here the agent to learn with you training interaction, reinforcement learning",
          "and they are developed.",
          "Darmstadt of course here Tio, minute-care, you immediately see what we are talking about.",
          "In hand manipulation via scoring with a reinforcement learning critique model,",
          "interesting for robotics or we go here with Heidelberg University, Mannheim, Germany.",
          "And they have here a zero-shot inference of dynamic system,",
          "preserving the long-term statistics.",
          "So this is an interesting interplay here, with a mixture of expert architecture,",
          "pre-trained for a dynamical system reconstruction.",
          "So if you are here into timelines, this is it.",
          "Plus, a new research here by Bielefeld University,",
          "Hello Bielefeld.",
          "And we have here the question whether the algorithms, the machine learning algorithms are",
          "in an ever-increasing importance to our society.",
          "So interesting referencing here to the European Union AI Act.",
          "So there's a lot of research, how to implement this and what about fairness?",
          "We go on, we have here Bielefeld University again, beautiful.",
          "May 20, 25, 25, 2, day as you see and they go about building and utilizing dynamic",
          "partner models for an adaptive explanation generation.",
          "And you see we're working here with our colleagues in the social cognitive systems,",
          "constructing explainability.",
          "This is interesting. So you see, sometimes the focal point on a particular day,",
          "are really not where we have the US American focal points or the development points where",
          "China has here. It's focused interesting.",
          "Talking about specific German focal points, what about a modern G-Burt,",
          "a German only one billion free-trained,",
          "a parameter encoder model trained from scratch."
        ]
      },
      {
        "type": "slide",
        "start_time": "0.0:5.0:59",
        "start_index": 181,
        "offset": 359.8221094674556,
        "source": "Output\\Best AI Papers of Today German Edition (Deutschland)\\thumbnails\\hi\\thumb-0182.png",
        "Video_Content": "We introduce a novel continuous encoder-decoder architecture that uses an epsilon- constrained kernel and learns to apply the convolution operator to adaptive and optimized query points. 2505.12944v1 [cs.LG] 19 May 2025 arXiv CALM-PDE: for Latent Space Modeling of Time-dependent PDEs Jan Hagnberger* Daniel Mathias Machine Learning and Simulation Lab, Institute for Artificial Intelligence, University of Stuttgart Max School for Intelligent Systems Center for Simulation Science (SimTech) Abstract Solving time-dependent Partial Differential Equations (PDEs) using a densely discretized domain is a fundamental problem in various scientific and engi- neering disciplines, climate phenomena and fluid dynamics. However, performing these directly in the physical space often incurs significant \u2018computational several neural surrogate models operate latent space to solve the PDE. approaches reduce computational complexity, they often Transformer-based irregularly sampled domains, resulting in memory contrast, neural networks allow memory-efficient encoding tind decoding but are limited to regular by: these propose CALM-PDE, a model discretized compressed latent novel continuous convolution-based encoder-decoder architecture that kernel and to apply the convolution operator and optimized query points. We demon- strate the effectiveness.of CALM-PDE set of PDEs With regularly and sampled spatial domains, CALM-PDE is performs existing baseline methods offering significant improvements in memory and to Transformer-based Introduction Many scientific and fluid mechanics, rely on simulating physical systems, often, involving solying spatio-temporal Partial Differential Equations (PDEs). In recent years, (ML) have the of PDEs (Lu &",
        "Audio_Content": [
          "This is great icing every nation should have its own language more in its own native language.",
          "Why if you have multiple languages, at least one of your dominant languages,",
          "I don't think that everything should be done in English.",
          "So great if you have a diversification, if you go in your",
          "modern tongue in your national primary language and you develop AI in your",
          "for your society, in your language modern bird.",
          "Yeah, they don't go with the classical bird system that have in this",
          "in the innovation of the modern bird system. And if you want, yes, of course I have a video on modern",
          "bird. Then we have here where we are a max plank, of course, Stuttgart Center for",
          "Simulation Science, Simulation Science, Simulation of Science, okay, University of Stuttgart",
          "and they have your continuous and a collaborative convolution for latent space modeling",
          "of time dependent partial differential equations.",
          "Real nice, really interesting, you know exactly what we're talking",
          "for talking about Stuttgart. And they say, hey, we introduce a",
          "now continuous convolution-based encoder decoder architecture."
        ]
      },
      {
        "type": "slide",
        "start_time": "0.0:7.0:11",
        "start_index": 217,
        "offset": 431.3889378698225,
        "source": "Output\\Best AI Papers of Today German Edition (Deutschland)\\thumbnails\\hi\\thumb-0218.png",
        "Video_Content": "documen 2505.13036v1 [cs.CL] May 2025 arXiv KIT\u2019s Offline Speech Translation and Instruction Following Submission for IWSLT 2025 Sai Maike Thai-Binh Nguyen, Seymanur Akti, Jan Niehues, Alexander Waibel Karlsruhe Institute of Technology Abstract The scope of the International Workshop on Spoken Language Translation has re- cently broadened beyond traditional Speech Translation (ST) to encompass a wider array of tasks, including Speech Question Answer- ing and Summarization. This shift is partly driven by the growing capabilities of modern systems, particularly with the success of Large Language Models (LLMs). In this paper, we present the Karlsruhe Institute of submissions for the Offline ST and Instruc- tion Following (IF) tracks, where we leverage LLMs to enhance performance across all tasks. For the Offline ST track, we propose a pipeline that employs multiple automatic speech recog- nition systems, whose outputs are fused using an LLM with document-level context. This is followed by a two-step translation process, incorporating additional refinement step to im- prove translation quality. For the IF track, we develop an end-to-end model that integrates a speech encoder with an LLM to perform a wide range of instruction-following tasks. We complement it with a final document-level re- finement stage to further enhance output quality by using contextual information. Introduction This paper provides an overview of the systems A growing research trend in the field is the ap- plication of Large Language Models (LLMs) to speech processing tasks (Tang et al., 2023; Ziifle and Niehues, 2024; Chu et al., 2024b; Abouelenin et al., 2025, among others), leveraging their strong general knowledge and natural language under- standing capabilities. These strengths make LLMs particularly relevant to both the Offline ST and IF tracks. Accordingly, in our submissions, we ex- plore strategies for effectively integrating LLMs into speech processing pipelines. There are multiple approaches to leveraging LLMs in speech systems. One strategy involves incorporating LLMs as an additional step within a cascaded architecture (Koneru et al., 2024a), where they can perform task-specific refinement. This modular approach allows each component to be trained independently, benefiting from specialized data. Alternatively, LLMs can be integrated in an end-to-end fashion (Tang et al., 2023; Ziifle and Niehues, 2024; Chu et Abouelenin et al., 2025), allowing for better information flow and po- tentially improving generalization to unseen tasks. Although both the Offline and IF tasks fall un- der the umbrella of speech processing, they differ significantly in nature. In the offline setting, speed and adaptability to unseen tasks are not primary concerns. In contrast, the IF task demands flexibil-",
        "Audio_Content": [
          "So I like a T5 that we know from five years ago.",
          "They'd use a Cian-Abscyline neighborhood constraint corno and learns to apply",
          "the convolution operator to adapt if an optimized, very point.",
          "Real interested if you are into the optimization of partial differential equation",
          "and their applications.",
          "Real interesting is here, technical University of Munich, yes, and University of Rome.",
          "This is beautiful.",
          "Natalize two buyers on up to bias, detecting bias here in use,",
          "use publication articles, whatever TV news with a bias detector.",
          "So there's a lot of focal point here on fair reporting that every society group is really",
          "also here. Have access is here represented in a fair and open way and also they want to detect",
          "if there are any biases from any group in any particular online publication.",
          "Can we detect the bias and this is nice and they perform here.",
          "And this is so beautiful because they go back to the roots.",
          "No remember birth and sentence birth.",
          "So they tell us here in their latest publication, we perform a sentence-level bias classification",
          "by fine tuning a Roberto-based model. My goodness, last time I worked with Roberto was about three",
          "years ago on an expert annotated.",
          "Hey, babe data set. Oh, this sounds interesting. So you see, they care a lot about the",
          "media bias that might encounter you if you are looking here in the internet or whatever TV or",
          "whatever you go. They say, hey, we want to make sure everybody has here access to the real truth.",
          "Here we go on in a thing. This is for the last one, Karlsruhe Institute of Technology,",
          "get beautiful and they go here in this particular publication.",
          "Please, this is only a snapshot of one day.",
          "And they are more than one thousand paper that AI filtered out because it's not in my area of expertise.",
          "And they say, hey, we here here in doing the research on a pipeline that employs multiple",
          "automatic speech recognition systems whose outputs that then fuse with a large language model",
          "with document-level context. And this is followed up by two step translation process.",
          "In corporing, now additional refinements step to improve the translation quality,",
          "but simply providing the additional context to your two-distranslation exercise.",
          "Really interesting, kid, one of the beautiful institutions in Germany. And I think this is it.",
          "Just out of scope, this is just here an impulsive video generation. I just wanted to show you",
          "if you search for partners, if you want to find here, people that are also working in",
          "a particular field of AI. I thought, hey, today was so such a strong signal here in the publication",
          "of AI research, German Research and German Institution are now really a strong force here,",
          "especially I think in Europe. Maybe I will do one of my next videos here on France and France.",
          "And I've seen also in Scandinavia, there are some real interesting research group emerging now.",
          "Of course, it is not about the institution, but it is about the human, the people, the highly,",
          "I don't know, educated talents that we have. And wherever there are, if you want to go,",
          "I don't know for one or two years, somewhere in Saudi Arabia, if you want to change here,",
          "or if you come from the US to Europe. Just want to give you a feeling about what are here.",
          "Today, just in one day in this snapshot, the research areas where people do the AI research",
          "where they published where they're interested in developing no system. And yeah, this was just here",
          "for my friends in Germany. I think this is absolutely beautiful. Have such a strong signal",
          "of artificial intelligence here today in the heart of Europe. And yeah, if you're interested in this,",
          "why not subscribe and I'll see you in one of my next videos."
        ]
      }
    ]
  }
}